{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQiEuS9NRs5NxcG6yUfEwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcdevaney/pyAMPACTtutorials/blob/main/04_pyAMPACT_audio_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>pyAMPACT Performance Data Calculations</h1>"
      ],
      "metadata": {
        "id": "qEMh3qjCeHvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd_p93OXdjr4",
        "outputId": "bd56db36-4050-47c9-bbf9-bbf803560213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyAMPACTtutorials'...\n",
            "remote: Enumerating objects: 433, done.\u001b[K\n",
            "remote: Counting objects: 100% (256/256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 433 (delta 155), reused 182 (delta 106), pack-reused 177\u001b[K\n",
            "Receiving objects: 100% (433/433), 19.45 MiB | 16.31 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n",
            "Importing libraries...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jcdevaney/pyAMPACTtutorials.git\n",
        "from IPython.utils import io\n",
        "print('Importing libraries...')\n",
        "with io.capture_output() as captured:\n",
        "    !pip install --upgrade pandas\n",
        "    !pip install -i https://test.pypi.org/simple/ --no-deps pyampact\n",
        "    import pyampact\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and align symbolic and audio files (as in 01-pyAMPACT_introduction.ipynb for details)"
      ],
      "metadata": {
        "id": "C7JwtEeJ2saE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '/content/pyAMPACTtutorials/test_files/aveMaria_seg.wav'\n",
        "score_file = '/content/pyAMPACTtutorials/test_files/aveMaria_seg.mei'\n",
        "\n",
        "y, original_sr = librosa.load(audio_file)\n",
        "\n",
        "piece = pyampact.Score(score_file)\n",
        "\n",
        "\n",
        "target_sr = 4000\n",
        "win_ms = 100\n",
        "hop_length = 32\n",
        "width = 3\n",
        "n_harm = 3\n",
        "\n",
        "res, dtw, spec, nmat = pyampact.run_alignment(\n",
        "    y, original_sr, piece, piece.nmats(), width, target_sr, n_harm, win_ms, hop_length)"
      ],
      "metadata": {
        "id": "CHkCzvE6dmvM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's examaine the contents of data_compilation.py"
      ],
      "metadata": {
        "id": "gYMHTjz4V0iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyampact.alignmentUtils import f0_est_weighted_sum_spec\n",
        "from pyampact.performance import estimate_perceptual_parameters\n",
        "\n",
        "# index into audio using piece.nmats() to identify the beginning and ending of\n",
        "# each note (this using code copied from the data_compilation function)\n",
        "# then estimate each of the performance parameters using estimate_perceptual_parameters\n",
        "\n",
        "nmat = piece.nmats()\n",
        "\n",
        "y, original_sr = librosa.load(audio_file)\n",
        "\n",
        "all_note_vals = []\n",
        "all_note_ids = []\n",
        "\n",
        "for key, df in nmat.items():\n",
        "\n",
        "    midiList = np.array(nmat[key]['MIDI'])\n",
        "    loc = 1\n",
        "    f0 = []\n",
        "    pwr = []\n",
        "    t = []\n",
        "    M = []\n",
        "    xf = []\n",
        "\n",
        "    ons = res['on']\n",
        "    offs = res['off']\n",
        "\n",
        "    note_vals = []\n",
        "    note_ids = []\n",
        "\n",
        "    # # ons = np.nonzero(estimatedOns)e[0]\n",
        "    # # offs = np.nonzero(estimatedOffs)[0]\n",
        "    for loc in range(len(ons)):\n",
        "        #Estimate f0 for a matrix (or vector) of amplitudes and frequencies\n",
        "        [f0, pwr, t, M, xf] = f0_est_weighted_sum_spec(audio_file, ons[loc], offs[loc], midiList[loc], y, original_sr);\n",
        "        # Estimate note-wise perceptual values\n",
        "        note_vals.append(estimate_perceptual_parameters(f0, pwr, M, original_sr, 256, 1))\n",
        "        note_ids.append(nmat[key].index[loc])\n",
        "    all_note_vals.append(note_vals)\n",
        "    all_note_ids.append(note_ids)\n",
        "\n",
        "# calculate paramters for the first note\n",
        "i = 0\n",
        "loc = 0\n",
        "\n",
        "df.loc[i,'f0Vals'] = str(all_note_vals[loc][i]['f0_vals'])\n",
        "\n",
        "# Mean f0\n",
        "df.loc[i,'meanf0'] = np.mean(all_note_vals[loc][i]['f0_vals'])\n",
        "\n",
        "# Perceived pitch based on\n",
        "# H. Gockel, B. C. Moore, and R. P. Carlyon, “Influence\n",
        "# of rate of change of frequency on the overall pitch of\n",
        "# frequency-modulated tones,” The Journal of the Acoustical\n",
        "# Society of America, vol. 109, no. 2, pp. 701–712,665 2001\n",
        "df.loc[i,'ppitch1'] = all_note_vals[loc][i]['ppitch'][0]\n",
        "df.loc[i,'ppitch2'] = all_note_vals[loc][i]['ppitch'][1]\n",
        "\n",
        "# Jitter calculated from the difference between sequential\n",
        "# frame-wise f0 estimates.\n",
        "df.loc[i,'jitter'] = all_note_vals[loc][i]['jitter']\n",
        "\n",
        "# Vibrato descriptors are calculated by first computing the spectrum of the\n",
        "# note-segmented f0 trace with an FFT.\n",
        "\n",
        "# Vibrato depth is estimated by doubling the maximum absolute value in the FFT\n",
        "df.loc[i,'vibratoDepth'] = all_note_vals[loc][i]['vibrato_depth']\n",
        "\n",
        "# Vibrato rate is estimated by finding the position of the maximum\n",
        "# absolute value in the FFT.\n",
        "df.loc[i,'vibratoRate'] = all_note_vals[loc][i]['vibrato_rate']\n",
        "\n",
        "df.loc[i,'pwrVals'] = str(all_note_vals[loc][i]['pwr_vals'])\n",
        "\n",
        "# Mean power is calculated from From the note-level frame-wise power\n",
        "# estimates using the arithmetic mean.\n",
        "df.loc[i,'meanPwr'] = np.mean(all_note_vals[loc][i]['pwr_vals'])\n",
        "\n",
        "# Shimmer (\\(S\\)) is approximated an analogous manner to  Jitter,\n",
        "# but by by calculating difference between sequential frame-wise\n",
        "# power estimates\n",
        "df.loc[i,'shimmer'] = all_note_vals[loc][i]['shimmer']\n",
        "\n",
        "#Timbre is estimated from the spectral representation used to calculate\n",
        "# the frame-wise \\fo{} and power estimates for each note.\n",
        "# The relevant \\texttt{librosa} functions are run on these\n",
        "# spectrogram-like representations to calculate spectral centroid,\n",
        "# spectral flux, spectral slope, and spectral flatness.\n",
        "df.loc[i,'specCentVals'] = str(all_note_vals[loc][i]['spec_centroid'])\n",
        "df.loc[i,'meanSpecCent'] = np.mean(all_note_vals[loc][i]['spec_centroid'])\n",
        "df.loc[i,'specBandwidthVals'] = str(all_note_vals[loc][i]['spec_bandwidth'])\n",
        "df.loc[i,'meanSpecBandwidth'] = np.mean(all_note_vals[loc][i]['spec_bandwidth'])\n",
        "df.loc[i,'specContrastVals'] = str(all_note_vals[loc][i]['spec_contrast'])\n",
        "df.loc[i,'meanSpecContrast'] = np.mean(all_note_vals[loc][i]['spec_contrast'])\n",
        "df.loc[i,'specFlatnessVals'] = str(all_note_vals[loc][i]['spec_flatness'])\n",
        "df.loc[i,'meanSpecFlatness'] = np.mean(all_note_vals[loc][i]['spec_flatness'])\n",
        "df.loc[i,'specRolloffVals'] = str(all_note_vals[loc][i]['spec_rolloff'])\n",
        "df.loc[i,'meanSpecRolloff'] = np.mean(all_note_vals[loc][i]['spec_rolloff'])\n",
        "\n",
        "#You can calculates other parameters from the f0, power, or\n",
        "# magnitude spectrum chunks here"
      ],
      "metadata": {
        "id": "2OgqZl2y3lGO"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}